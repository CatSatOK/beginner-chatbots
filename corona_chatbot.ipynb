{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71db5ac7-a7c1-4eaa-adaa-6d1fe1a3f171",
   "metadata": {},
   "source": [
    "Chatbot that will answer users questions about covid by returning the most similar phrases found a text file of information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c7fc4560-6f42-4461-b23e-cdbb349b8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd60e63-4008-4f6a-91df-33757f151f3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting the info from the document\n",
    "with open('infos_corona.txt','r',errors = 'ignore', encoding = \"utf8\") as f:\n",
    "    texte = f.read()\n",
    "\n",
    "#deleting the nca acronym\n",
    "texte = re.sub('n.c.a.', 'nca', texte)\n",
    "\n",
    "#tokenisation\n",
    "phrases_token = nltk.sent_tokenize(texte, language = \"french\")\n",
    "\n",
    "# removing the questions (as we want answers)\n",
    "for i in sorted(range(len(phrases_token)), reverse = True):\n",
    "    if re.search(r\"\\?\", phrases_token[i]):\n",
    "        del phrases_token[i]\n",
    "\n",
    "#removing duplicates\n",
    "phrases_token = list(set(phrases_token)) \n",
    "\n",
    "#cleaning function\n",
    "def nettoyage(texte):\n",
    "    texte = texte.lower()\n",
    "    #standardising the visus name to coronavirus\n",
    "    texte = re.sub('covid-19| virus|covid |sars-cov', 'coronavirus', texte)\n",
    "    #further cleaning functions\n",
    "    texte = re.sub('coronavirus coronavirus', 'coronavirus', texte)\n",
    "    texte = re.sub(f\"[{string.punctuation}]\", \" \", texte)\n",
    "    texte = re.sub('[éèê]', 'e', texte)\n",
    "    texte = re.sub('[àâ]', 'a', texte)\n",
    "    texte = re.sub('[ô]', 'o', texte)\n",
    "    texte = re.sub('mort(\\w){0,3}|deces|deced(\\w){1,5}', 'deces', texte)\n",
    "    texte = re.sub('remedes?|traitements?|antidotes?', 'traitement', texte)\n",
    "    texte = re.sub('medec(\\w){1,5}|medic(\\w){1,3}', 'medical', texte)\n",
    "    return texte\n",
    "\n",
    "#getting the cleaned functions\n",
    "phrases_nettoyees = []\n",
    "for i in range(len(phrases_token)):\n",
    "    phrases_nettoyees.append(nettoyage(phrases_token[i]))\n",
    "      \n",
    "#getting the french stop words\n",
    "from stop_words import get_stop_words\n",
    "french_stop_words = get_stop_words('french')\n",
    "\n",
    "#applying the stemmer to get root of word\n",
    "from nltk.stem.snowball import FrenchStemmer\n",
    "french_stem = FrenchStemmer()\n",
    "\n",
    "def stem_tokens(tokens):\n",
    "    return [french_stem.stem(token) for token in tokens]\n",
    "    \n",
    "def stem_norm(text):\n",
    "    return stem_tokens(nltk.word_tokenize(text))\n",
    "\n",
    "# generating responses from hte tf-idf matrix\n",
    "tf_idf = TfidfVectorizer(tokenizer=stem_norm, token_pattern=None, stop_words = french_stop_words)\n",
    "tf_idf_chat = tf_idf.fit(phrases_nettoyees)\n",
    "\n",
    "#response function\n",
    "def reponse_corona(user_sentence):\n",
    "    user_sentence = [user_sentence]\n",
    "    phrases_tf = tf_idf_chat.transform(phrases_nettoyees)\n",
    "    user_tf = tf_idf_chat.transform(user_sentence)\n",
    "    similarity = cosine_similarity(user_tf, phrases_tf).flatten()\n",
    "    index_max_sim = np.argmax(similarity)\n",
    "    if(similarity[index_max_sim] == 0):\n",
    "        robo_response = \"Je n'ai pas trouvé cette information, désolé!\"\n",
    "    elif(similarity[index_max_sim] <= 0.30):\n",
    "        robo_response = \"\"\"Je ne suis pas sûr d'avoir trouvé exactement ce que vous vouliez dire, voilà ce que j'ai trouvé : \\n\"\"\"+phrases_token[index_max_sim] \n",
    "    else:\n",
    "        simil_index = []\n",
    "        for i in range(len(similarity)):\n",
    "            if similarity[i] > 0.3:\n",
    "                simil_index.append(i)\n",
    "        robo_response = '\\n'.join([phrases_token[i] for i in simil_index])\n",
    "    return robo_response\n",
    "\n",
    "# greetings\n",
    "salutations_user = r\"bonjour.*|salut.*|hello.*|hey.*|coucou.*|bonsoir.*\"\n",
    "salutations_robot = [\"Bonjour, bienvenue sur ce chatbot!\"]\n",
    "def salutations(user_sentence):\n",
    "    if re.fullmatch(salutations_user, user_sentence):\n",
    "        return random.choice(salutations_robot)\n",
    "    \n",
    "#question answering\n",
    "nouvelles_user = r\".*[çs]a va.*\\?|.*la pêche\\?|.*la forme\\?\"\n",
    "nouvelles_robot = [\"Je suis un robot, ça va jamais vraiment\",\n",
    "                   \"Un peu marre du confinement\",\n",
    "                   \"On fait aller!\",\n",
    "                   \"J'ai une pêche d'enfer!!!\"]\n",
    "def nouvelles(user_sentence):\n",
    "    if re.fullmatch(nouvelles_user, user_sentence):\n",
    "        return random.choice(nouvelles_robot)\n",
    "    \n",
    "#exiting terms\n",
    "exit_user = [\"au revoir\", \"bye\", \"bye bye\", \"à +\", \"ciao\"]\n",
    "exit_bot = [\"au revoir!\", \"j'espère vous avoir été utile!\",\"à une prochaine fois :)\"]\n",
    "\n",
    "#creating the chatbot \n",
    "flag = True\n",
    "print(\"\"\"> Corona-bot : Je suis le corona-bot, je réponds à vos questions sur l'épidémie ! \n",
    "Si vous voulez ajouter des infos supplémentaires (de sources vérifiées bien sûr !) vous pouvez taper 'infos'.\n",
    "Pour quitter, vous pouvez juste me dire au revoir.\"\"\")\n",
    "while flag:\n",
    "    user_sentence = input(\"> Vous :  \")\n",
    "    user_sentence = user_sentence.lower()\n",
    "    if (user_sentence == \"infos\"):\n",
    "        user_info = input(\"> Vos informations :  \")\n",
    "        phrases_token.append(user_info)\n",
    "        user_info = user_info.lower()\n",
    "        user_info = nettoyage(user_info)\n",
    "        phrases_nettoyees.append(user_info) \n",
    "        tf_idf_chat = tf_idf.fit(phrases_nettoyees)\n",
    "        print(\"Merci c'est noté!\")\n",
    "    elif not (user_sentence in exit_user):\n",
    "        if (salutations(user_sentence) != None):\n",
    "            print(\"> Corona-bot : \" + salutations(user_sentence))\n",
    "        elif (nouvelles(user_sentence) != None):\n",
    "            print(\"> Corona-bot : \" + nouvelles(user_sentence))\n",
    "        else:\n",
    "            user_sentence = nettoyage(user_sentence)\n",
    "            print(\"> Corona-bot : \" + reponse_corona(user_sentence))\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"> Corona-bot : \" + random.choice(exit_bot))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
